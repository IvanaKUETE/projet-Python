{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"border: thick double #32a1ce; text-align:center;border-radius:35px\">\n",
    "Modélisation statistiques\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a new path pointing to the current working directory\n",
    "HOME_DIR = Path.cwd().parent\n",
    "\n",
    "# create a variable for data directory\n",
    "DATA_DIR = Path(HOME_DIR, \"machine learning\")\n",
    "\n",
    "print(f\"Work directory: {HOME_DIR} \\nData directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = pd.read_csv(Path(DATA_DIR, \"data_cleaned.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"border: thick double #32a1ce; text-align:center;border-radius:35px\">\n",
    "I- feature scaling des variables\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables de notre jeu de données ayant une très grande échelle, nous allons faire du feature scaling (les remettre à l'echelle) pour rendre nos données homogènes. La méthode privilégiée dans notre cas est la standardisation car les méthodes (regression) utilisées reposent sur l'hypothèse de normalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_num = data_cleaned.select_dtypes(['int64','float64']).columns\n",
    "col_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Var_cat=data_cleaned.select_dtypes(['object']).columns\n",
    "Var_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_norm=[ 'NumberofFloors', 'PropertyGFATotal',\n",
    "       'SourceEUI(kBtu/sf)', 'SteamUse(kBtu)',\n",
    "       'Electricity(kBtu)', 'NaturalGas(kBtu)', 'TotalGHGEmissions',\n",
    "       'GHGEmissionsIntensity', 'NumberOfUse', 'BuildingAge',\n",
    "       'SiteEnergyUse(log(kBtu))']\n",
    "var_cat=['BuildingType', 'PrimaryPropertyType', 'Neighborhood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned[var_norm].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mod=data_cleaned[['NumberofFloors', 'PropertyGFATotal',\n",
    "       'SourceEUI(kBtu/sf)', 'SteamUse(kBtu)',\n",
    "       'Electricity(kBtu)', 'NaturalGas(kBtu)', 'TotalGHGEmissions',\n",
    "       'GHGEmissionsIntensity', 'NumberOfUse', 'BuildingAge',\n",
    "       'SiteEnergyUse(log(kBtu))','BuildingType', 'PrimaryPropertyType', 'Neighborhood']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style= \"text-align:left\">\n",
    "I.1- Standardisation  des variables numériques \n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style= \"text-align:left\">\n",
    "I.1.a-  Standarscaler \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation des variables numériques\n",
    "data_mod[var_norm] = StandardScaler().fit_transform(data_mod[var_norm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifiaction de la normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verification_normalisation(data_mod):\n",
    "      for variable_norm in data_mod :\n",
    "        data_mod_col=data_mod[variable_norm]\n",
    "        moyenne=np.mean(data_mod_col)\n",
    "        ecartType = np.std(data_mod_col)\n",
    "\n",
    "        print(\"les statistiques pour la variable {}\" .format(variable_norm))\n",
    "        print(\"La moyenne est de : {} \".format(round(abs(moyenne), 2)))\n",
    "        print(\"L'écart type est de : {} \".format(round(abs(ecartType), 2)))\n",
    "        print(\" \")\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_normalisation(data_cleaned[var_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h1 style= \"text-align:left\">\n",
    "Recherche du meilleur algorithme \n",
    "</h1>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
